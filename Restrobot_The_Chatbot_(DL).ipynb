{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "783995e2",
   "metadata": {},
   "source": [
    "# CHATBOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd82650",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "A chatbot is a computer program designed to simulate conversation with human users, especially over the internet. They are often used in customer service, virtual assistants, and various other applications to provide information, answer questions, and assist users in completing tasks. Chatbots can be simple or complex, with some using artificial intelligence (AI) and natural language processing (NLP) to understand and respond to user input in a more human-like manner. They are becoming increasingly popular due to their ability to automate interactions and provide efficient and personalized customer service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f9a4fb",
   "metadata": {},
   "source": [
    "# Applications:\n",
    "\n",
    "Chatbots have a wide range of applications across various industries. Some common applications include:\n",
    "\n",
    "1. **Customer Service:** Chatbots are used by businesses to provide instant and round-the-clock customer support. They can answer FAQs, resolve simple issues, and escalate complex problems to human agents.\n",
    "\n",
    "\n",
    "2. **E-commerce:** Chatbots in e-commerce can assist customers in finding products, placing orders, tracking shipments, and handling returns or exchanges.\n",
    "\n",
    "\n",
    "3. **Healthcare:** In healthcare, chatbots can help patients schedule appointments, receive medication reminders, access health information, and even provide initial diagnosis based on symptoms.\n",
    "\n",
    "\n",
    "4. **Banking and Finance:** Chatbots in banking and finance can help users check account balances, transfer funds, pay bills, and get information about financial products and services.\n",
    "\n",
    "\n",
    "5. **Travel and Hospitality:** Chatbots can assist travelers with booking flights, hotels, and rental cars, as well as provide travel tips, destination information, and real-time updates on travel disruptions.\n",
    "\n",
    "\n",
    "6. **Education:** Chatbots can be used in education to provide students with learning resources, answer academic queries, schedule classes, and provide feedback on assignments.\n",
    "\n",
    "\n",
    "7. **HR and Recruitment:** Chatbots can streamline the recruitment process by screening candidates, scheduling interviews, and answering questions about job openings and company policies.\n",
    "\n",
    "\n",
    "8. **Restaurant:** In the restaurant sector, chatbots can be used to automate reservations, allowing customers to book tables, view menus, and get information about the restaurant's location and operating hours.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59efa44",
   "metadata": {},
   "source": [
    "# Application on `Deep Learning` models into the `Chatbot`:\n",
    "\n",
    "1. **Artificial Neural Network (ANN)**: The code uses a type of ANN called a Multilayer Perceptron (MLP). An MLP consists of multiple layers of nodes, each connected to nodes in the adjacent layers. These nodes, or neurons, process inputs and pass the result to the next layer. The MLP learns patterns in the input data through a process called backpropagation, adjusting the weights of connections between neurons to minimize errors in predictions.\n",
    "\n",
    "\n",
    "2. **Natural Language Processing (NLP)**: NLP techniques are employed to preprocess the text data. This includes:\n",
    "   - **Tokenization**: Breaking down sentences into individual words or tokens.\n",
    "   - **Lemmatization**: Reducing words to their base or root form. For example, \"running\" becomes \"run\".\n",
    "   - **Normalization**: Converting all text to lowercase to ensure consistency.\n",
    "\n",
    "\n",
    "3. **Training the Model**: The preprocessed text data is used to train the ANN. Each sentence is represented as a \"bag of words,\" a numerical vector indicating the presence or absence of each word in the sentence. The output layer of the ANN has neurons corresponding to different intents, and the model is trained to predict the correct intent for a given input sentence.\n",
    "\n",
    "\n",
    "4. **Predicting Intents**: Once the model is trained, it can predict the intent of new user messages. The input message is preprocessed using the same NLP techniques, converted into a bag of words, and fed into the ANN. The output neuron with the highest activation represents the predicted intent, and the corresponding response is selected from the predefined responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dfde8f",
   "metadata": {},
   "source": [
    "## Additional algorithms used in the model:\n",
    "A simple feedforward neural network is implemented in the model using the Keras library. The neural network consists of several dense (fully connected) layers with **rectified linear unit (ReLU) activation functions**. The model is trained using **stochastic gradient descent (SGD)** with Nesterov accelerated gradient as the optimizer and categorical cross-entropy as the loss function. The final layer uses a softmax activation function to predict the intent of the user input. This architecture is commonly used for text classification tasks, such as intent recognition in chatbots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891eca52",
   "metadata": {},
   "source": [
    "# Dataset Used\n",
    "\n",
    "The dataset contains various intents for a **restaurant chatbot**, along with patterns and responses for each intent. Here are a few key points about the dataset:\n",
    "\n",
    "1. **Intents**: Each intent represents a category of user queries or statements that the chatbot can recognize and respond to.\n",
    "\n",
    "\n",
    "2. **Patterns**: Patterns are example phrases or sentences that users might type or say to the chatbot to express a particular intent.\n",
    "\n",
    "\n",
    "3. **Responses**: Responses are the messages that the chatbot should output when it recognizes a specific intent in the user input.\n",
    "\n",
    "\n",
    "4. **Tags**: Tags are labels assigned to each intent. They are used to identify which intent a particular user input corresponds to.\n",
    "\n",
    "\n",
    "5. **Examples**:\n",
    "   **intents**\n",
    "   - **tag**: `greeting`\n",
    "     - **Patterns**: [\"Hi\", \"Hey\", \"Hello\"]\n",
    "     - **Responses**: [\"Hello I'm Restrobot! How can I help you?\", \"Hi! I'm Restrobot. How may I assist you today?\"]\n",
    "\n",
    "   - **tag**: `book_table`\n",
    "     - **Patterns**: [\"Book a table\", \"Can I book a table?\", \"I want to book a table\"]\n",
    "     - **Responses**: [\"Yes, You can book a table\"]\n",
    "\n",
    "   - **tag**: `menu`\n",
    "     - **Patterns**: [\"Id like to order something\", \"whats on the menu\", \"could i get something to eat\"]\n",
    "     - **Responses**: [\"We have both veg and non-veg dish, In Vegetarian we have: Vegetable Biryani, Paneer Tikka, Aloo Gobi, Chana Masala, Palak Paneer, Fried Rice, Dal Makhani, Veg Pulao, Veg Korma, Masala Dosa, Veg Manchurian. In Non-Vegetarian we have: Chicken Biryani, Butter Chicken, Tandoori Chicken, Fish Curry, Mutton Rogan Josh, Chicken Tikka Masala, Egg Fried Rice, Prawn Curry, Chicken 65, Chicken Korma. In extras we have: Pizza, Soup, Burger, Fried chicken\"]\n",
    "\n",
    "The dataset is designed to help the chatbot understand and respond to a variety of user queries related to booking tables, checking menu options, getting information about the restaurant, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312585ab",
   "metadata": {},
   "source": [
    "#### Install libraries if required\n",
    "- pip install tensorflow\n",
    "- pip install keras\n",
    "- pip install pickle\n",
    "- pip install nltk\n",
    "- nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e14656e",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b117282c-b879-44d2-9190-d6fe4c189da1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee7efda",
   "metadata": {},
   "source": [
    "# Preparing the Data for Chatbot Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "813d866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "\n",
    "data_file = open(r\"D:\\WEBEL_Assignments\\Machine_&_Deep_learning_RJSir\\Projects\\Project_11_Chatbot_DL\\Secondary Dataset\\dataset.json\").read()\n",
    "intents = json.loads(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23ed90f",
   "metadata": {},
   "source": [
    "This code segment initializes empty lists for words, classes, and documents, and sets up a list of ignored words. It then reads a JSON file containing chatbot intents, storing them in a variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744145bf",
   "metadata": {},
   "source": [
    "# Printing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06ede7fc-3801-458a-bac9-a8c5b0c04025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'greeting',\n",
       "   'patterns': ['Hi',\n",
       "    'Hey',\n",
       "    'Hello',\n",
       "    'Good morning!',\n",
       "    'Hey! Good morning',\n",
       "    'Hey there',\n",
       "    'Hey Janet',\n",
       "    'Very good morning',\n",
       "    'A very good morning to you',\n",
       "    'Greeting',\n",
       "    'Greetings to you'],\n",
       "   'responses': [\"Hello I'm Restrobot! How can I help you?\",\n",
       "    \"Hi! I'm Restrobot. How may I assist you today?\"]},\n",
       "  {'tag': 'book_table',\n",
       "   'patterns': ['Book a table',\n",
       "    'Can I book a table?',\n",
       "    'I want to book a table',\n",
       "    'Book seat',\n",
       "    'I want to book a seat',\n",
       "    'Can I book a seat?',\n",
       "    'Could you help me book a table',\n",
       "    'Can I reserve a seat?',\n",
       "    'I need a reservation',\n",
       "    'Can you help me with a reservation',\n",
       "    'Can I book a reservation',\n",
       "    'Can i have a table?',\n",
       "    'Help me reserve a table',\n",
       "    'book table'],\n",
       "   'responses': ['Yes, You can book a table']},\n",
       "  {'tag': 'available_tables',\n",
       "   'patterns': ['How many seats are available?',\n",
       "    'Available seats',\n",
       "    'How many tables are available?',\n",
       "    'Available tables',\n",
       "    'Are there any tables available?',\n",
       "    'What is the capacity of the restaurant',\n",
       "    'Are there any available tables?',\n",
       "    'Are there any seats left?',\n",
       "    'I wanted to know if there are any tables available now',\n",
       "    'May I know if you have any tables which I can book?'],\n",
       "   'responses': ['Yes, we have a table']},\n",
       "  {'tag': 'goodbye',\n",
       "   'patterns': ['cya',\n",
       "    'I will leave now',\n",
       "    'See you later',\n",
       "    'Goodbye',\n",
       "    'Leaving now, Bye',\n",
       "    'Good bye dear',\n",
       "    'Bye dear',\n",
       "    'I am Leaving',\n",
       "    'Have a Good day',\n",
       "    'cya later',\n",
       "    'I gotta go now',\n",
       "    'I gotta rush now',\n",
       "    'Thank you, bye',\n",
       "    'Bye',\n",
       "    'Ok Bye',\n",
       "    'Okay goodnight',\n",
       "    'Have a good day ahead',\n",
       "    'Have a great day',\n",
       "    'Tata',\n",
       "    'Take care'],\n",
       "   'responses': [\"It's been my pleasure serving you!\",\n",
       "    'Hope to see you again soon! Goodbye!',\n",
       "    'Bye! Hope to see you again!']},\n",
       "  {'tag': 'identity',\n",
       "   'patterns': ['what is your name',\n",
       "    'what should I call you',\n",
       "    'whats your name?',\n",
       "    'who are you',\n",
       "    'Are you human?',\n",
       "    'Am i talking to a bot',\n",
       "    'Are you a bot',\n",
       "    'Can i have your name please',\n",
       "    'name'],\n",
       "   'responses': ['You can call me Restrobot.',\n",
       "    \"I'm Restrobot!\",\n",
       "    \"I'm Restrobot.\"]},\n",
       "  {'tag': 'hours',\n",
       "   'patterns': ['when are you guys open',\n",
       "    'what are your hours',\n",
       "    'hours of operation',\n",
       "    'hours',\n",
       "    'what is the timing',\n",
       "    'when are you open',\n",
       "    'Are you open on all days?',\n",
       "    'are you open now',\n",
       "    'are you open on holidays',\n",
       "    'are you guys open on all weekdays?',\n",
       "    'working hours',\n",
       "    'hours',\n",
       "    'what are your working hours?'],\n",
       "   'responses': ['We are open 10am-12am Monday-Friday!']},\n",
       "  {'tag': 'menu',\n",
       "   'patterns': ['Id like to order something',\n",
       "    'whats on the menu',\n",
       "    'could i get something to eat',\n",
       "    'Im damn hungry',\n",
       "    'I am hungry',\n",
       "    'Show me the menu',\n",
       "    'What food do you have',\n",
       "    'wHat food are you offering?',\n",
       "    'whats on the menu today?',\n",
       "    'Let me see the menu',\n",
       "    'menu'],\n",
       "   'responses': ['We have both veg and non-veg dish, In Vegetarian we have: Vegetable Biryani, Paneer Tikka, Aloo Gobi, Chana Masala, Palak Paneer, Fried Rice, Dal Makhani, Veg Pulao, Veg Korma, Masala Dosa, Veg Manchurian. In Non-Vegetarian we have: Chicken Biryani, Butter Chicken, Tandoori Chicken, Fish Curry, Mutton Rogan Josh, Chicken Tikka Masala, Egg Fried Rice, Prawn Curry, Chicken 65, Chicken Korma. In extras we have: Pizza, Soup, Burger, Fried chicken. We also have: Cold Drink, Ice-creams, Juice, Deserts']},\n",
       "  {'tag': 'contact',\n",
       "   'patterns': ['contact information',\n",
       "    'how do we contact you',\n",
       "    'how can i contact you',\n",
       "    'can i get the contact details',\n",
       "    'I wanna give some feedback',\n",
       "    'how can i give some feedback?',\n",
       "    'CAn you give me the contact of an executive?',\n",
       "    'What is the help desk phone number?',\n",
       "    'Can you give me your number',\n",
       "    'Can i get the customer care number?',\n",
       "    'CAn I get help desk number'],\n",
       "   'responses': ['You can contact us at contact@aindrisrestro.com, our help desk number is 8787878787']},\n",
       "  {'tag': 'address',\n",
       "   'patterns': ['what is the location?',\n",
       "    'whats the location',\n",
       "    'where are you located?',\n",
       "    'where is the restaurant located?',\n",
       "    'address',\n",
       "    'whats the address?',\n",
       "    'what is the address of the restaurant',\n",
       "    'I am not able to locate you',\n",
       "    'I cant find your location',\n",
       "    'Can i have the address of the restaurant',\n",
       "    'how to reach there?',\n",
       "    'Address?',\n",
       "    'what is the address of this restaurant?'],\n",
       "   'responses': [\"You can locate us at Aindri's Restro, Phase 1, Rd Number 6, Whitefield, Kolkata, WestBengal, 700066\"]},\n",
       "  {'tag': 'positive_feedback',\n",
       "   'patterns': ['the noodles were amazing',\n",
       "    'i loved the food',\n",
       "    'you did a good job',\n",
       "    'Love the food',\n",
       "    'Really love it',\n",
       "    'Love the staff behavior',\n",
       "    'my son devoured the brownie!',\n",
       "    'pizza was so cheesy!',\n",
       "    'perfectly baked',\n",
       "    'we are very satisfied with the service',\n",
       "    'the soup was a real game changer',\n",
       "    'such delicious flavours',\n",
       "    'so glad we discovered this place',\n",
       "    'Me and my family is very satisfied with this service and food',\n",
       "    'this place is awesome'],\n",
       "   'responses': ['Thankyou for the feedback']},\n",
       "  {'tag': 'negative_feedback',\n",
       "   'patterns': ['what is wrong with these noodles?',\n",
       "    'The choco lava was so undercooked',\n",
       "    'Ew such a waste of money man',\n",
       "    'Prices are too high honestly',\n",
       "    'I hate the menu, such less options',\n",
       "    'too salty',\n",
       "    'we were served cold food',\n",
       "    'so disappointed',\n",
       "    'the food is pathetic',\n",
       "    'hate it',\n",
       "    'my wife hates the food',\n",
       "    'eww',\n",
       "    'hate the staff behavior',\n",
       "    'i hate it',\n",
       "    'hate the service',\n",
       "    'please train the staff properly',\n",
       "    'This was such a waste of money',\n",
       "    'Hate the staff',\n",
       "    'Extremely dissatisfied',\n",
       "    'disappointed',\n",
       "    'so bad',\n",
       "    'very bad',\n",
       "    'disgusting food'],\n",
       "   'responses': ['I am sorry, We will try to improve the dish.']},\n",
       "  {'tag': 'sanitation',\n",
       "   'patterns': ['is it really safe to eat here?',\n",
       "    'Could you tell your COVID safety protocols?',\n",
       "    'I would like to know about the cleanliness of the restaurant',\n",
       "    'Please share your sanitization process',\n",
       "    'I am concerned about the COVID related sanitization',\n",
       "    'Is it safe to eat out in this pandemic?',\n",
       "    'are you clean',\n",
       "    'i am concerned about the cleanliness'],\n",
       "   'responses': ['I understand your concern. Here are the WHO recommended COVID protocols we follow to ensure your safety: \\n 1. All our staff are double masked 24x7. \\n 2. All our staff is checked for fevers and other symptoms daily. \\n 3. All surfaces are frequently sanitized. \\n 4. We use this friendly bot to reduce physical closeness to the least!']},\n",
       "  {'tag': 'offers',\n",
       "   'patterns': ['Could you tell me the pocket friendly options?',\n",
       "    'Are there any discounts going on?',\n",
       "    'Are there any special offers today?',\n",
       "    'What about the festive offers?',\n",
       "    'Could you please tell me which foods are on discount?',\n",
       "    'are there any discounts',\n",
       "    'are there any discount offers',\n",
       "    'do you have any offers?',\n",
       "    'what are the offers going on?',\n",
       "    'what are the discounts available?'],\n",
       "   'responses': ['Yes, we can give you a 10% discount.']},\n",
       "  {'tag': 'veg_enquiry',\n",
       "   'patterns': ['Can I see the vegetarian options?',\n",
       "    'Do you have any vegetarian options??',\n",
       "    'Please show me your best vegetarian foods',\n",
       "    'I dont want to eat non veg',\n",
       "    'I am vegetarian',\n",
       "    'vegetarian',\n",
       "    'is this place vegetarian?'],\n",
       "   'responses': ['For our vegetarian guests we have a special vegetarian menu']},\n",
       "  {'tag': 'recipe_enquiry',\n",
       "   'patterns': ['Could you tell me more about recipe of this dish?',\n",
       "    'What is the recipe of this dish?',\n",
       "    'what are the ingredients of this dish?',\n",
       "    'tell me the recipe'],\n",
       "   'responses': ['We have used special ingridents in this dish.']},\n",
       "  {'tag': 'suggest',\n",
       "   'patterns': ['what do you recommend?',\n",
       "    'do you have any suggestions',\n",
       "    'please suggest something',\n",
       "    \"why don't you recommend me a dish\",\n",
       "    'help me choose what to order',\n",
       "    'Surprise me!',\n",
       "    'Do you have any special recommendations for me?',\n",
       "    'What do you suggest?',\n",
       "    'what is your suggestion',\n",
       "    'what is a must try',\n",
       "    'what should i try',\n",
       "    'what should i eat?',\n",
       "    'tell me what to order',\n",
       "    'tell me what i should buy',\n",
       "    'recommend me a dish',\n",
       "    'suggest me a dish'],\n",
       "   'responses': ['You can try Veg Manchurian and Fried Rice']},\n",
       "  {'tag': 'general',\n",
       "   'patterns': ['okay', 'sure', 'cool', 'hmm', 'fine', 'thanks', 'uhuh'],\n",
       "   'responses': [':)',\n",
       "    'Glad to serve you!',\n",
       "    'Happy to help!',\n",
       "    'Always happy to assist you!']},\n",
       "  {'tag': 'payments',\n",
       "   'patterns': ['Can i pay with credit card',\n",
       "    'Cash',\n",
       "    'Google Pay',\n",
       "    'Apple Pay',\n",
       "    'AMEX',\n",
       "    'Debit',\n",
       "    'Paypal'],\n",
       "   'responses': ['We accept the following payment methods: Cash, VISA, Master, AMEX, PayPal, Apple Pay, Google Pay'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'food',\n",
       "   'patterns': ['menu',\n",
       "    'food of the day',\n",
       "    'suggestion',\n",
       "    'recommendation',\n",
       "    'eat'],\n",
       "   'responses': ['Today we can recommend you the Paneer Rice Plate, you will love it :)',\n",
       "    'Today, the restaurant chef recommends you to try the Paneer Rice Plate',\n",
       "    'This week we recommend you to try one of the following plates: Soyabean Biryani, Veg Biryani or Panner Tikka Masala'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'delivery',\n",
       "   'patterns': ['ordering', 'home', 'delivery'],\n",
       "   'responses': ['Food and beverages can be delivered to your home. The delivery fee is 60Rs.'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'bye',\n",
       "   'patterns': ['bye', 'see you', 'see ya'],\n",
       "   'responses': ['We hope you are satisfied with our service. Have a good day!',\n",
       "    'We are looking forward to welcome you again in our restaurant'],\n",
       "   'context_set': ''}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762e72c1",
   "metadata": {},
   "source": [
    "# Tokenizing Patterns and Creating the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f33b52f7-db80-4aac-9cee-595c41ad487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        \n",
    "        #tokenize each word\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        \n",
    "        #add documents in the corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8123ff8",
   "metadata": {},
   "source": [
    "1. **Loop through Intents**: The code iterates over each dictionary in the `intents['intents']` list, where each dictionary represents an intent for the chatbot.\n",
    "\n",
    "\n",
    "2. **Tokenization**: For each intent, it further iterates over the 'patterns' key, which contains a list of text patterns associated with that intent. For each pattern, the text is tokenized into individual words using `nltk.word_tokenize(pattern)`.\n",
    "\n",
    "\n",
    "3. **Updating Words List**: The tokenized words (`w`) are then added to the `words` list using `words.extend(w)`. This list will contain all the unique words in the entire set of patterns, forming the vocabulary for the chatbot.\n",
    "\n",
    "\n",
    "4. **Creating Documents**: Each pattern along with its associated intent tag is appended as a tuple to the `documents` list. This `documents` list will be used as the training data for the chatbot.\n",
    "\n",
    "\n",
    "5. **Updating Classes List**: The intent tag is also added to the `classes` list if it is not already present. This list will contain all the unique intent tags, representing the different categories the chatbot can classify messages into."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fe2b7e",
   "metadata": {},
   "source": [
    "# Using `WordNet` to Find Synonyms and Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b511866f-1503-4303-af04-cedf35583a00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a building where people go to eat\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "# Example: Find synonyms for the word \"dog\"\n",
    "synonyms = wn.synsets('restaurant')\n",
    "print(synonyms[0].definition())  # Prints the definition of the first synonym for 'dog'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e01fa",
   "metadata": {},
   "source": [
    "The code uses the **WordNet** module from **NLTK** to find synonyms for the word \"restaurant.\" WordNet is a lexical database of the English language that groups words into sets of synonyms (synsets) and provides short definitions for these words. \n",
    "\n",
    "1. `wn.synsets('restaurant')`: This function call retrieves all the synsets (sets of synonyms) for the word \"restaurant\" from WordNet.\n",
    "\n",
    "\n",
    "2. `synonyms[0].definition()`: This line accesses the first synset (index 0) from the list of synsets found for the word \"restaurant\" and prints its definition. This definition provides a brief description of the meaning of the word in that context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ec28b",
   "metadata": {},
   "source": [
    "# Lemmatizing, Sorting, and Saving Words and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "522a23b2-0395-4bef-ae2d-73a9678ffb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 documents\n",
      "21 classes ['address', 'available_tables', 'book_table', 'bye', 'contact', 'delivery', 'food', 'general', 'goodbye', 'greeting', 'hours', 'identity', 'menu', 'negative_feedback', 'offers', 'payments', 'positive_feedback', 'recipe_enquiry', 'sanitation', 'suggest', 'veg_enquiry']\n",
      "262 unique lemmatized words [',', 'a', 'able', 'about', 'address', 'ahead', 'all', 'am', 'amazing', 'amex', 'an', 'and', 'any', 'apple', 'are', 'available', 'awesome', 'bad', 'baked', 'behavior', 'best', 'book', 'bot', 'brownie', 'buy', 'bye', 'call', 'can', 'cant', 'capacity', 'card', 'care', 'cash', 'changer', 'cheesy', 'choco', 'choose', 'clean', 'cleanliness', 'cold', 'concerned', 'contact', 'cool', 'could', 'covid', 'credit', 'customer', 'cya', 'damn', 'day', 'dear', 'debit', 'delicious', 'delivery', 'desk', 'detail', 'devoured', 'did', 'disappointed', 'discount', 'discovered', 'disgusting', 'dish', 'dissatisfied', 'do', 'dont', 'eat', 'ew', 'eww', 'executive', 'extremely', 'family', 'feedback', 'festive', 'find', 'fine', 'flavour', 'food', 'for', 'friendly', 'game', 'get', 'give', 'glad', 'go', 'going', 'good', 'goodbye', 'goodnight', 'google', 'got', 'great', 'greeting', 'guy', 'hate', 'have', 'hello', 'help', 'here', 'hey', 'hi', 'high', 'hmm', 'holiday', 'home', 'honestly', 'hour', 'how', 'human', 'hungry', 'i', 'id', 'if', 'im', 'in', 'information', 'ingredient', 'is', 'it', 'janet', 'job', 'know', 'later', 'lava', 'le', 'leave', 'leaving', 'left', 'let', 'like', 'locate', 'located', 'location', 'love', 'loved', 'man', 'many', 'may', 'me', 'menu', 'money', 'more', 'morning', 'must', 'my', \"n't\", 'na', 'name', 'need', 'non', 'noodle', 'not', 'now', 'number', 'of', 'offer', 'offering', 'ok', 'okay', 'on', 'open', 'operation', 'option', 'order', 'ordering', 'out', 'pandemic', 'pathetic', 'pay', 'paypal', 'perfectly', 'phone', 'pizza', 'place', 'please', 'pocket', 'price', 'process', 'properly', 'protocol', 'reach', 'real', 'really', 'recipe', 'recommend', 'recommendation', 'related', 'reservation', 'reserve', 'restaurant', 'rush', 'safe', 'safety', 'salty', 'sanitization', 'satisfied', 'seat', 'see', 'served', 'service', 'share', 'should', 'show', 'so', 'some', 'something', 'son', 'soup', 'special', 'staff', 'such', 'suggest', 'suggestion', 'sure', 'surprise', 'ta', 'table', 'take', 'talking', 'tata', 'tell', 'thank', 'thanks', 'the', 'there', 'these', 'this', 'timing', 'to', 'today', 'too', 'train', 'try', 'uhuh', 'undercooked', 'veg', 'vegetarian', 'very', 'wa', 'wan', 'want', 'wanted', 'waste', 'we', 'weekday', 'were', 'what', 'whats', 'when', 'where', 'which', 'who', 'why', 'wife', 'will', 'with', 'working', 'would', 'wrong', 'ya', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "# lemmatize, lower each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "# documents = combination between patterns and intents\n",
    "print (len(documents), \"documents\")\n",
    "\n",
    "# classes = intents\n",
    "print (len(classes), \"classes\", classes)\n",
    "\n",
    "# words = all words, vocabulary\n",
    "print (len(words), \"unique lemmatized words\", words)\n",
    "pickle.dump(words,open('words.pkl','wb'))\n",
    "pickle.dump(classes,open('classes.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04923625",
   "metadata": {},
   "source": [
    "1. **Lemmatization**: Each word in the `words` list is lemmatized (converted to its base or root form), converted to lowercase, and stored back in the `words` list, excluding words in the `ignore_words` list.\n",
    "\n",
    "\n",
    "2. **Removing Duplicates**: The `words` list is converted to a set to remove duplicate words, then converted back to a list and sorted alphabetically.\n",
    "\n",
    "\n",
    "3. **Sorting Classes**: The `classes` list (containing unique tags or intents) is also converted to a set to remove duplicates, then converted back to a list and sorted alphabetically.\n",
    "\n",
    "\n",
    "4. **Printing Summary**: The number of documents (combinations of patterns and intents), the number of unique classes (intents), and the number of unique lemmatized words in the vocabulary are printed.\n",
    "\n",
    "\n",
    "5. **Pickling**: The `words` list and `classes` list are saved as pickle files (`words.pkl` and `classes.pkl` respectively) for later use. Pickling is a way to serialize and save Python objects to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb96a78",
   "metadata": {},
   "source": [
    "# Creating and Preparing the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fee39942-42cc-4eae-b2ee-b267d061eb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Shape of bag: (262,)\n",
      "Shape of output_row: (21,)\n",
      "Training data created\n"
     ]
    }
   ],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # lemmatize each word - create base word, in attempt to represent related words\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array with 1, if word match found in current pattern\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "# output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# shuffle our features and turn into np.array\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Assuming 'words', 'classes', 'documents', and 'lemmatizer' are defined elsewhere\n",
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    pattern_words = doc[0]\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    \n",
    "    # Initialize output row inside the loop\n",
    "    output_row = list(output_empty)\n",
    "\n",
    "    for w in words:\n",
    "        # Check if word is in the pattern_words\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "        \n",
    "    # Set the corresponding index of the output_row to 1\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    # Append the bag and output_row as a tuple to training\n",
    "    training.append((bag, output_row))\n",
    "\n",
    "# Shuffle training data before converting to numpy array\n",
    "random.shuffle(training)\n",
    "\n",
    "# Print the shapes of each element in the training list\n",
    "for item in training:\n",
    "    print(\"Shape of bag:\", np.array(item[0]).shape)\n",
    "    print(\"Shape of output_row:\", np.array(item[1]).shape)\n",
    "\n",
    "# Separate features (X) and labels (Y) from the training data\n",
    "train_x = np.array([bag for bag, _ in training])\n",
    "train_y = np.array([output_row for _, output_row in training])\n",
    "\n",
    "print(\"Training data created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d630aa11",
   "metadata": {},
   "source": [
    "1. **Creating Training Data**: A list called `training` is initialized to store the training data. An empty array `output_empty` is created with the length equal to the number of classes. For each document in the `documents` list, a bag of words is created for the sentence using the words in the `words` list. This bag of words is a binary representation where 1 indicates the presence of a word in the sentence, and 0 indicates absence.\n",
    "\n",
    "\n",
    "2. **Creating Output Rows**: An output row is created for each document, where the index corresponding to the class of the document is set to 1, and all other indices remain 0. This represents the expected output for the neural network.\n",
    "\n",
    "\n",
    "3. **Shuffling and Converting to Numpy Array**: The training data is shuffled to ensure that the model does not learn any sequence patterns. Each element in the `training` list is converted into a tuple containing the bag of words and the output row. These tuples are then converted into numpy arrays `train_x` and `train_y`, representing the features (X) and labels (Y) for the training data respectively.\n",
    "\n",
    "\n",
    "4. **Printing Shapes**: The shapes of each bag of words and output row in the training data are printed to verify that the data is structured correctly.\n",
    "\n",
    "\n",
    "5. **Result**: The final output is the message \"Training data created,\" indicating that the training data preparation process is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b52721",
   "metadata": {},
   "source": [
    "# Creating and Training the Chatbot Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1eba75fd-4f83-4365-844b-0c3bd421a374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 3.0367 - accuracy: 0.0682\n",
      "Epoch 2/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 2.9174 - accuracy: 0.1318\n",
      "Epoch 3/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 2.7466 - accuracy: 0.1909\n",
      "Epoch 4/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 2.6745 - accuracy: 0.2318\n",
      "Epoch 5/200\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 2.4357 - accuracy: 0.3136\n",
      "Epoch 6/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 2.2147 - accuracy: 0.3955\n",
      "Epoch 7/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 2.0552 - accuracy: 0.4227\n",
      "Epoch 8/200\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 1.8629 - accuracy: 0.4727\n",
      "Epoch 9/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 1.7004 - accuracy: 0.5091\n",
      "Epoch 10/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 1.4930 - accuracy: 0.5909\n",
      "Epoch 11/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 1.3380 - accuracy: 0.6727\n",
      "Epoch 12/200\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 1.2173 - accuracy: 0.6636\n",
      "Epoch 13/200\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 1.1448 - accuracy: 0.6364\n",
      "Epoch 14/200\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 1.1093 - accuracy: 0.6500\n",
      "Epoch 15/200\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.9317 - accuracy: 0.7227\n",
      "Epoch 16/200\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.9478 - accuracy: 0.7045\n",
      "Epoch 17/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.7415 - accuracy: 0.7682\n",
      "Epoch 18/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.7596 - accuracy: 0.7636\n",
      "Epoch 19/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.6640 - accuracy: 0.8273\n",
      "Epoch 20/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.6868 - accuracy: 0.8045\n",
      "Epoch 21/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.6226 - accuracy: 0.8273\n",
      "Epoch 22/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.5569 - accuracy: 0.8545\n",
      "Epoch 23/200\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.4576 - accuracy: 0.8591\n",
      "Epoch 24/200\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.5253 - accuracy: 0.8273\n",
      "Epoch 25/200\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.4348 - accuracy: 0.8682\n",
      "Epoch 26/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.4151 - accuracy: 0.8727\n",
      "Epoch 27/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.4075 - accuracy: 0.8727\n",
      "Epoch 28/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.4073 - accuracy: 0.9000\n",
      "Epoch 29/200\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.3970 - accuracy: 0.8955\n",
      "Epoch 30/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.4393 - accuracy: 0.8636\n",
      "Epoch 31/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.3583 - accuracy: 0.9091\n",
      "Epoch 32/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.3796 - accuracy: 0.8818\n",
      "Epoch 33/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2860 - accuracy: 0.9136\n",
      "Epoch 34/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.3172 - accuracy: 0.9000\n",
      "Epoch 35/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2730 - accuracy: 0.9273\n",
      "Epoch 36/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2594 - accuracy: 0.9227\n",
      "Epoch 37/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2814 - accuracy: 0.9227\n",
      "Epoch 38/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2986 - accuracy: 0.9182\n",
      "Epoch 39/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2946 - accuracy: 0.9136\n",
      "Epoch 40/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2176 - accuracy: 0.9455\n",
      "Epoch 41/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2491 - accuracy: 0.9227\n",
      "Epoch 42/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2631 - accuracy: 0.9136\n",
      "Epoch 43/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2548 - accuracy: 0.9409\n",
      "Epoch 44/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2043 - accuracy: 0.9318\n",
      "Epoch 45/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2410 - accuracy: 0.9273\n",
      "Epoch 46/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2598 - accuracy: 0.9136\n",
      "Epoch 47/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.3159 - accuracy: 0.9045\n",
      "Epoch 48/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2512 - accuracy: 0.9182\n",
      "Epoch 49/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1593 - accuracy: 0.9500\n",
      "Epoch 50/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2086 - accuracy: 0.9318\n",
      "Epoch 51/200\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1474 - accuracy: 0.9727\n",
      "Epoch 52/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2083 - accuracy: 0.9273\n",
      "Epoch 53/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1308 - accuracy: 0.9591\n",
      "Epoch 54/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1323 - accuracy: 0.9682\n",
      "Epoch 55/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1346 - accuracy: 0.9682\n",
      "Epoch 56/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1844 - accuracy: 0.9409\n",
      "Epoch 57/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2228 - accuracy: 0.9227\n",
      "Epoch 58/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1795 - accuracy: 0.9318\n",
      "Epoch 59/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1497 - accuracy: 0.9636\n",
      "Epoch 60/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1518 - accuracy: 0.9591\n",
      "Epoch 61/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1889 - accuracy: 0.9455\n",
      "Epoch 62/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1487 - accuracy: 0.9636\n",
      "Epoch 63/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1597 - accuracy: 0.9500\n",
      "Epoch 64/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1597 - accuracy: 0.9455\n",
      "Epoch 65/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1686 - accuracy: 0.9545\n",
      "Epoch 66/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1195 - accuracy: 0.9727\n",
      "Epoch 67/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1329 - accuracy: 0.9545\n",
      "Epoch 68/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1717 - accuracy: 0.9545\n",
      "Epoch 69/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1568 - accuracy: 0.9500\n",
      "Epoch 70/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1738 - accuracy: 0.9500\n",
      "Epoch 71/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1009 - accuracy: 0.9682\n",
      "Epoch 72/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1408 - accuracy: 0.9455\n",
      "Epoch 73/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1406 - accuracy: 0.9636\n",
      "Epoch 74/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1185 - accuracy: 0.9636\n",
      "Epoch 75/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1163 - accuracy: 0.9591\n",
      "Epoch 76/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1553 - accuracy: 0.9364\n",
      "Epoch 77/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1225 - accuracy: 0.9682\n",
      "Epoch 78/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1250 - accuracy: 0.9545\n",
      "Epoch 79/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1508 - accuracy: 0.9318\n",
      "Epoch 80/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1393 - accuracy: 0.9500\n",
      "Epoch 81/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1157 - accuracy: 0.9773\n",
      "Epoch 82/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1420 - accuracy: 0.9545\n",
      "Epoch 83/200\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9545\n",
      "Epoch 84/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1130 - accuracy: 0.9545\n",
      "Epoch 85/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1148 - accuracy: 0.9682\n",
      "Epoch 86/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1129 - accuracy: 0.9727\n",
      "Epoch 87/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0611 - accuracy: 0.9909\n",
      "Epoch 88/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1358 - accuracy: 0.9545\n",
      "Epoch 89/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0814 - accuracy: 0.9818\n",
      "Epoch 90/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1059 - accuracy: 0.9591\n",
      "Epoch 91/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0554 - accuracy: 0.9864\n",
      "Epoch 92/200\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.0555 - accuracy: 0.9909\n",
      "Epoch 93/200\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9773\n",
      "Epoch 94/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1003 - accuracy: 0.9773\n",
      "Epoch 95/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1451 - accuracy: 0.9500\n",
      "Epoch 96/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1022 - accuracy: 0.9591\n",
      "Epoch 97/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1161 - accuracy: 0.9545\n",
      "Epoch 98/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1132 - accuracy: 0.9682\n",
      "Epoch 99/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1172 - accuracy: 0.9636\n",
      "Epoch 100/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0709 - accuracy: 0.9909\n",
      "Epoch 101/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0987 - accuracy: 0.9591\n",
      "Epoch 102/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0989 - accuracy: 0.9727\n",
      "Epoch 103/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0606 - accuracy: 0.9818\n",
      "Epoch 104/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1264 - accuracy: 0.9591\n",
      "Epoch 105/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1158 - accuracy: 0.9455\n",
      "Epoch 106/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1025 - accuracy: 0.9591\n",
      "Epoch 107/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1022 - accuracy: 0.9682\n",
      "Epoch 108/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1163 - accuracy: 0.9591\n",
      "Epoch 109/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0838 - accuracy: 0.9727\n",
      "Epoch 110/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1190 - accuracy: 0.9591\n",
      "Epoch 111/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0925 - accuracy: 0.9682\n",
      "Epoch 112/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1280 - accuracy: 0.9545\n",
      "Epoch 113/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0630 - accuracy: 0.9864\n",
      "Epoch 114/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0679 - accuracy: 0.9636\n",
      "Epoch 115/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1264 - accuracy: 0.9455\n",
      "Epoch 116/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0977 - accuracy: 0.9591\n",
      "Epoch 117/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0881 - accuracy: 0.9727\n",
      "Epoch 118/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1045 - accuracy: 0.9636\n",
      "Epoch 119/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0706 - accuracy: 0.9682\n",
      "Epoch 120/200\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.0835 - accuracy: 0.9682\n",
      "Epoch 121/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0739 - accuracy: 0.9682\n",
      "Epoch 122/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1139 - accuracy: 0.9682\n",
      "Epoch 123/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0924 - accuracy: 0.9727\n",
      "Epoch 124/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0515 - accuracy: 0.9818\n",
      "Epoch 125/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0899 - accuracy: 0.9727\n",
      "Epoch 126/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0775 - accuracy: 0.9682\n",
      "Epoch 127/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1043 - accuracy: 0.9636\n",
      "Epoch 128/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0792 - accuracy: 0.9773\n",
      "Epoch 129/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0863 - accuracy: 0.9818\n",
      "Epoch 130/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0457 - accuracy: 0.9864\n",
      "Epoch 131/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0654 - accuracy: 0.9864\n",
      "Epoch 132/200\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.0675 - accuracy: 0.9682\n",
      "Epoch 133/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0751 - accuracy: 0.9773\n",
      "Epoch 134/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0764 - accuracy: 0.9727\n",
      "Epoch 135/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0869 - accuracy: 0.9591\n",
      "Epoch 136/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0774 - accuracy: 0.9773\n",
      "Epoch 137/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0830 - accuracy: 0.9818\n",
      "Epoch 138/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0495 - accuracy: 0.9818\n",
      "Epoch 139/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0397 - accuracy: 0.9909\n",
      "Epoch 140/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0621 - accuracy: 0.9818\n",
      "Epoch 141/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0993 - accuracy: 0.9773\n",
      "Epoch 142/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0514 - accuracy: 0.9864\n",
      "Epoch 143/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0394 - accuracy: 0.9955\n",
      "Epoch 144/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0749 - accuracy: 0.9727\n",
      "Epoch 145/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0775 - accuracy: 0.9773\n",
      "Epoch 146/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0866 - accuracy: 0.9727\n",
      "Epoch 147/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1007 - accuracy: 0.9591\n",
      "Epoch 148/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1181 - accuracy: 0.9591\n",
      "Epoch 149/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1166 - accuracy: 0.9636\n",
      "Epoch 150/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1290 - accuracy: 0.9727\n",
      "Epoch 151/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0909 - accuracy: 0.9591\n",
      "Epoch 152/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0643 - accuracy: 0.9682\n",
      "Epoch 153/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0738 - accuracy: 0.9727\n",
      "Epoch 154/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1021 - accuracy: 0.9636\n",
      "Epoch 155/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0762 - accuracy: 0.9727\n",
      "Epoch 156/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0969 - accuracy: 0.9636\n",
      "Epoch 157/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0745 - accuracy: 0.9682\n",
      "Epoch 158/200\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.0534 - accuracy: 0.9818\n",
      "Epoch 159/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0370 - accuracy: 0.9909\n",
      "Epoch 160/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0743 - accuracy: 0.9682\n",
      "Epoch 161/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0893 - accuracy: 0.9682\n",
      "Epoch 162/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0373 - accuracy: 0.9909\n",
      "Epoch 163/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0434 - accuracy: 0.9864\n",
      "Epoch 164/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0671 - accuracy: 0.9682\n",
      "Epoch 165/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0298 - accuracy: 0.9955\n",
      "Epoch 166/200\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.0502 - accuracy: 0.9818\n",
      "Epoch 167/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0458 - accuracy: 0.9909\n",
      "Epoch 168/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1047 - accuracy: 0.9682\n",
      "Epoch 169/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0625 - accuracy: 0.9864\n",
      "Epoch 170/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0967 - accuracy: 0.9682\n",
      "Epoch 171/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0591 - accuracy: 0.9818\n",
      "Epoch 172/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0697 - accuracy: 0.9818\n",
      "Epoch 173/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0651 - accuracy: 0.9682\n",
      "Epoch 174/200\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.0587 - accuracy: 0.9773\n",
      "Epoch 175/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0628 - accuracy: 0.9727\n",
      "Epoch 176/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0511 - accuracy: 0.9773\n",
      "Epoch 177/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0597 - accuracy: 0.9727\n",
      "Epoch 178/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0833 - accuracy: 0.9636\n",
      "Epoch 179/200\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.0662 - accuracy: 0.9727\n",
      "Epoch 180/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0855 - accuracy: 0.9773\n",
      "Epoch 181/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0651 - accuracy: 0.9773\n",
      "Epoch 182/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0561 - accuracy: 0.9773\n",
      "Epoch 183/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0454 - accuracy: 0.9864\n",
      "Epoch 184/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0622 - accuracy: 0.9818\n",
      "Epoch 185/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0692 - accuracy: 0.9727\n",
      "Epoch 186/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1034 - accuracy: 0.9682\n",
      "Epoch 187/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1020 - accuracy: 0.9591\n",
      "Epoch 188/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0773 - accuracy: 0.9727\n",
      "Epoch 189/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0700 - accuracy: 0.9727\n",
      "Epoch 190/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0474 - accuracy: 0.9818\n",
      "Epoch 191/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0588 - accuracy: 0.9773\n",
      "Epoch 192/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0603 - accuracy: 0.9727\n",
      "Epoch 193/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0422 - accuracy: 0.9864\n",
      "Epoch 194/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0432 - accuracy: 0.9864\n",
      "Epoch 195/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0537 - accuracy: 0.9773\n",
      "Epoch 196/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0620 - accuracy: 0.9773\n",
      "Epoch 197/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0633 - accuracy: 0.9818\n",
      "Epoch 198/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0655 - accuracy: 0.9727\n",
      "Epoch 199/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0830 - accuracy: 0.9636\n",
      "Epoch 200/200\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0795 - accuracy: 0.9727\n",
      "model created\n"
     ]
    }
   ],
   "source": [
    "# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
    "# equal to number of intents to predict output intent with softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
    "from keras.optimizers.legacy import SGD\n",
    "\n",
    "sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "#fitting and saving the model \n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "model.save('chatbot_model.h5', hist)\n",
    "\n",
    "print(\"model created\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2b16473",
   "metadata": {},
   "source": [
    "## ReLU:\n",
    "ReLU stands for Rectified Linear Unit. It is a type of activation function that is commonly used in neural networks, especially in deep learning models. ReLU is defined as ( f(x) = max(0, x)), which means it returns 0 for any negative input and returns the input value for any non-negative input.\n",
    "\n",
    "ReLU has become popular because of its simplicity and effectiveness in training deep neural networks. It helps in mitigating the vanishing gradient problem, which can occur in networks that use activation functions like sigmoid or tanh, by allowing the gradient to flow more directly during backpropagation. This can lead to faster and more effective training of deep neural networks.\n",
    "\n",
    "\n",
    "**The above code creates a neural network model using Keras for a chatbot application. The steps are:**\n",
    "\n",
    "\n",
    "1. **Model Architecture**: The model is initialized as a Sequential model. Three layers are added:\n",
    "   - The first layer has 128 neurons, takes input shape equal to the length of the input feature vector (bag of words), and uses the ReLU activation function.\n",
    "   - A dropout layer is added with a rate of 0.5 to prevent overfitting.\n",
    "   - The second layer has 64 neurons and uses the ReLU activation function.\n",
    "   - Another dropout layer is added.\n",
    "   - The output layer has a number of neurons equal to the number of unique classes (intents) in the training data and uses the softmax activation function to output probabilities for each class.\n",
    "\n",
    "\n",
    "2. **Model Compilation**: The model is compiled using the categorical crossentropy loss function, which is suitable for multi-class classification problems. The optimizer used is stochastic gradient descent (SGD) with Nesterov accelerated gradient, which is a variant of SGD known for its good results in training neural networks.\n",
    "\n",
    "\n",
    "3. **Model Training**: The model is trained using the `fit` method on the training data (`train_x` and `train_y`) for 200 epochs with a batch size of 5. The training process is verbose, meaning it will display progress information during training.\n",
    "\n",
    "\n",
    "4. **Saving the Model**: Once training is complete, the model is saved to a file named 'chatbot_model.h5' along with the training history (`hist`).\n",
    "\n",
    "\n",
    "5. **Print Statement**: Finally, a message is printed indicating that the model has been created.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f0efa8",
   "metadata": {},
   "source": [
    "# Loading the Trained Model and Data for Chatbot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adb8b4a0-49a0-4f10-8975-0e7fc2202581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('chatbot_model.h5')\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "intents = json.loads(open(r\"D:\\WEBEL_Assignments\\Machine_&_Deep_learning_RJSir\\Projects\\Project_11_Chatbot_DL\\Secondary Dataset\\dataset.json\").read())\n",
    "words = pickle.load(open('words.pkl','rb'))\n",
    "classes = pickle.load(open('classes.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846cbc81",
   "metadata": {},
   "source": [
    "# Functions for Cleaning, Tokenizing, and Predicting Intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a5e4e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "\n",
    "    # tokenize the pattern - split words into array\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # stem each word - create short form for word\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=True):\n",
    "\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "\n",
    "    # bag of words - matrix of N words, vocabulary matrix\n",
    "    bag = [0]*len(words) \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                # assign 1 if current word is in the vocabulary position\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "    return(np.array(bag))\n",
    "\n",
    "def predict_class(sentence, model):\n",
    "\n",
    "    # filter out predictions below a threshold\n",
    "    p = bow(sentence, words,show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
    "\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eff250",
   "metadata": {},
   "source": [
    "This code loads a pre-trained chatbot model along with its necessary components for processing and generating responses. The steps followed are:\n",
    "\n",
    "1. `nltk` and `WordNetLemmatizer` are imported for text preprocessing. `lemmatizer` is then initialized to lemmatize words.\n",
    "\n",
    "\n",
    "2. The `pickle` library is imported for loading saved Python objects, and `numpy` is imported for numerical operations.\n",
    "\n",
    "\n",
    "3. The `load_model` function from `keras.models` is used to load a pre-trained chatbot model stored in a file named `'chatbot_model.h5'`.\n",
    "\n",
    "\n",
    "4. The `json` module is imported to work with JSON data, and `intents` is loaded from a JSON file containing intents for the chatbot. This JSON file likely contains patterns, responses, and tags for different intents.\n",
    "\n",
    "\n",
    "5. The `words` and `classes` are loaded from pickle files (`'words.pkl'` and `'classes.pkl'` respectively) that were previously saved. These files likely contain the preprocessed words and classes (intents) used for training the chatbot model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff931f3",
   "metadata": {},
   "source": [
    "# Functions for Generating Chatbot Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91d64b2e-f6e9-4b8d-b981-69a9001a71b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResponse(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if(i['tag']== tag):\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def chatbot_response(text):\n",
    "    ints = predict_class(text, model)\n",
    "    res = getResponse(ints, intents)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f9c97",
   "metadata": {},
   "source": [
    "These functions are used to generate a response from the chatbot based on the predicted intent for a given input text. The methods of each function are:\n",
    "\n",
    "\n",
    "1. `getResponse(ints, intents_json)`: This function takes the predicted intents (`ints`) and the intents JSON object (`intents_json`) as input. It retrieves a random response from the intents JSON object based on the predicted intent. It loops through the list of intents, compares the tag of each intent with the predicted intent, and if they match, it selects a random response from that intent's responses. It then returns the selected response.\n",
    "\n",
    "\n",
    "2. `chatbot_response(text)`: This function takes a text input, predicts the intent for that input using the `predict_class` function (not shown), and then gets a response using the `getResponse` function. It essentially orchestrates the process of predicting intent and selecting a response to form the chatbot's reply.\n",
    "\n",
    "\n",
    "These functions, along with the `predict_class` function (which is assumed to be defined elsewhere), work together to process user input, determine the intent of the input, and provide an appropriate response from the chatbot's predefined set of responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c638fbc7",
   "metadata": {},
   "source": [
    "# Graphical User Interface (GUI) for the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc7dffba-27a3-47f3-85fd-3bf0db6e05be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "#Creating GUI with tkinter\n",
    "\n",
    "import tkinter as tk\n",
    "\n",
    "def send():\n",
    "    msg = EntryBox.get(\"1.0\",'end-1c').strip()\n",
    "    EntryBox.delete(\"0.0\",tk.END)\n",
    "    if msg != '':\n",
    "        ChatLog.config(state=tk.NORMAL)\n",
    "        ChatLog.insert(tk.END, \"You: \" + msg + '\\n\\n')\n",
    "        ChatLog.config(foreground=\"#442265\", font=(\"Verdana\", 12 ))\n",
    "        res = chatbot_response(msg)\n",
    "        ChatLog.insert(tk.END, \"Bot: \" + res + '\\n\\n')\n",
    "        ChatLog.config(state=tk.DISABLED)\n",
    "        ChatLog.yview(tk.END)\n",
    "\n",
    "base = tk.Tk()\n",
    "base.title(\"Restrobot The Chatbot\")\n",
    "\n",
    "# Automatically adjust the window size based on its content\n",
    "base.geometry(\"\")\n",
    "base.resizable(width=True, height=True)\n",
    "\n",
    "#Create Chat window\n",
    "ChatLog = tk.Text(base, bd=2, bg=\"white\", height=\"8\", width=\"50\", font=\"Arial\", highlightthickness=2, highlightbackground=\"grey\")\n",
    "ChatLog.config(state=tk.DISABLED)\n",
    "\n",
    "#Bind scrollbar to Chat window\n",
    "scrollbar = tk.Scrollbar(base, command=ChatLog.yview, cursor=\"heart\")\n",
    "ChatLog['yscrollcommand'] = scrollbar.set\n",
    "\n",
    "#Create Button to send message\n",
    "SendButton = tk.Button(base, font=(\"Verdana\",12,'bold'), text=\"Send\", width=\"12\", height=5,\n",
    "                        bd=0, bg=\"#32de97\", activebackground=\"#3c9d9b\",fg='#ffffff',\n",
    "                        command=send,highlightthickness=2, highlightbackground=\"grey\")\n",
    "\n",
    "#Create the box to enter message\n",
    "EntryBox = tk.Text(base, bd=2, bg=\"white\",width=\"29\", height=\"5\", font=\"Arial\", highlightthickness=2, highlightbackground=\"grey\")\n",
    "\n",
    "#Place all components on the screen using pack geometry manager\n",
    "scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "ChatLog.pack(side=tk.TOP, fill=tk.BOTH, expand=True)\n",
    "EntryBox.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "SendButton.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True)\n",
    "\n",
    "base.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b23a5",
   "metadata": {},
   "source": [
    "This code creates a simple chatbot **GUI** using **Tkinter**. Here's a brief explanation of each component:\n",
    "\n",
    "1. `ChatLog`: This is a text widget that displays the chat history. It is configured to be disabled (`state=tk.DISABLED`) to prevent users from editing the chat history directly.\n",
    "\n",
    "\n",
    "2. `scrollbar`: This is a scrollbar widget that is linked to the `ChatLog` widget to allow scrolling through the chat history.\n",
    "\n",
    "\n",
    "3. `EntryBox`: This is a text widget where users can enter their messages. It is used to capture user input.\n",
    "\n",
    "\n",
    "4. `SendButton`: This is a button widget that users can click to send their messages. It is linked to the `send` function, which processes the user input and updates the chat history.\n",
    "\n",
    "\n",
    "5. `send` function: This function reads the message from the `EntryBox`, clears the `EntryBox`, and then updates the `ChatLog` with the user's message and the bot's response.\n",
    "\n",
    "\n",
    "6. `base.mainloop()`: This starts the Tkinter event loop, which is required to display the GUI and handle user interactions.\n",
    "\n",
    "\n",
    "This code creates a basic chatbot interface that allows users to interact with the chatbot by typing messages and receiving responses displayed in the chat history."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
